{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서의 분류\n",
    "\n",
    "\n",
    "- 주어진 문서에 대해 미리 정의된 클래스로 분류하는 작업\n",
    "- 예를 들어 뉴스에서 정치, 경제, 연예, 스포츠 등을 나누는 것\n",
    "- 메일의 내용을 분석해 스팸 메일의 여부를 결정하는 것\n",
    "- 자연어 처리 및 텍스트 마이닝에서 가장 기본적이고, 활용범위가 넓은 분아다.\n",
    "\n",
    "\n",
    "## 학습 모델\n",
    "\n",
    "\n",
    "- 분류는 지도합습에 속하며 로직스틱회귀분석을 비롯해 결정트리, 나이브베이즈, SVM 등 다양한 머신러인 방법이 존재한다.\n",
    "- 이 중 특히 나이브 베이즈는 텍스트 분류에 특화됐다고할 정도로 많이 사용된다.\n",
    "- 문서 분류는 지도학습에 속하므로 모든 문서 혹슨 텍스트에 라벨 혹은 미리 분류된 클래스가 있어야 한다.\n",
    "\n",
    "\n",
    "## 연습 데이터\n",
    "\n",
    "\n",
    "- 20 뉴스그룹 데이터셋은 문서 분류의 성능을 측정하기 위해 많이 사용되는 데이터셋이다.\n",
    "- 이는 유즈넷에 의존하는데, 유즈넷은 초창기 인터넷에서 이메일과 함께 가장 많이 사용되던 일종의 게시판이다.\n",
    "- 유즈넷에는 여러 늇 서버가 있고, 사용자는 이 중 한 서버에 접속해서 하나 이상의 뉴스그룹을 구독할 수 있다.\n",
    "- 뉴스그릅마다 특정 주제가 있으며, 사용자는 그에 맞는 뉴스를 올리거나 읽을 수 있다.\n",
    "- 여기 실습에서는 문제를 단순하게 함으로써 속도를 높이기 위해 총 20개의 토픽(카테고리) 중 4개만 선택한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# train 기준 데이터\n",
    "data = fetch_20newsgroups(subset=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  2034\n",
      "test:  1353\n",
      "topics:  ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
      "labels:  {0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "# 데이터 칼럼 확인\n",
    "categories = [\"alt.atheism\", \"talk.religion.misc\", \"comp.graphics\", \"sci.space\"]\n",
    "\n",
    "# 학습 데이터셋 로드\n",
    "# 문자 내용에 포함된 헤더나 푸터, 특수 문자 등을 제외하고 순수 내용만 분류\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"), categories=categories\n",
    ")\n",
    "\n",
    "# 테스트 데이터셋 로드\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset=\"test\", remove=(\"headers\", \"footers\", \"quotes\"), categories=categories\n",
    ")\n",
    "\n",
    "print(\"train: \", len(newsgroups_train.data))\n",
    "print(\"test: \", len(newsgroups_test.data))\n",
    "print(\"topics: \", newsgroups_train.target_names)\n",
    "print(\"labels: \", set(newsgroups_train.target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 텍스트 샘플:  Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "훈련 세트 라벨 샘플:  1\n",
      "테스트 세트 텍스트 샘플:  TRry the SKywatch project in  Arizona.\n",
      "테스트 세트 라벨 샘플:  2\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 값 확인\n",
    "print(\"훈련 세트 텍스트 샘플: \", newsgroups_train.data[0])\n",
    "print(\"훈련 세트 라벨 샘플: \", newsgroups_train.target[0])\n",
    "print(\"테스트 세트 텍스트 샘플: \", newsgroups_test.data[0])\n",
    "print(\"테스트 세트 라벨 샘플: \", newsgroups_test.target[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련세트 크기 (2034, 2000)\n",
      "테스트세트 크기 (1353, 2000)\n"
     ]
    }
   ],
   "source": [
    "# 카운트 기반 특성 추출\n",
    "X_train = newsgroups_train.data\n",
    "y_train = newsgroups_train.target\n",
    "\n",
    "X_test = newsgroups_test.data\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#  min_df: 최소 작성 갯수 즉 5개 미만 제외\n",
    "# max_df: 많은 문서에서 나타나는 단어 제외 50퍼 초과 삭제\n",
    "cv = CountVectorizer(max_features=2000, min_df=5, max_df=0.5)\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "print(\"훈련세트 크기\", X_train_cv.shape)\n",
    "\n",
    "X_test_cv = cv.transform(X_test)\n",
    "print(\"테스트세트 크기\", X_test_cv.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 : 0\n",
      "000 : 0\n",
      "01 : 0\n",
      "04 : 0\n",
      "05 : 0\n",
      "10 : 0\n",
      "100 : 0\n",
      "1000 : 0\n",
      "11 : 0\n",
      "12 : 0\n",
      "128 : 0\n",
      "129 : 0\n",
      "13 : 0\n",
      "130 : 0\n",
      "14 : 0\n",
      "15 : 0\n",
      "16 : 0\n",
      "17 : 0\n",
      "18 : 0\n",
      "19 : 0\n",
      "1987 : 0\n",
      "1988 : 0\n",
      "1989 : 0\n",
      "1990 : 0\n",
      "1991 : 0\n",
      "1992 : 0\n",
      "1993 : 0\n",
      "20 : 0\n",
      "200 : 0\n",
      "202 : 0\n",
      "21 : 0\n",
      "22 : 0\n",
      "23 : 0\n",
      "24 : 0\n",
      "25 : 0\n",
      "256 : 0\n",
      "26 : 0\n",
      "27 : 0\n",
      "28 : 0\n",
      "2d : 0\n",
      "30 : 0\n",
      "300 : 0\n",
      "31 : 0\n",
      "32 : 0\n",
      "33 : 0\n",
      "34 : 0\n",
      "35 : 0\n",
      "39 : 0\n",
      "3d : 0\n",
      "40 : 0\n",
      "400 : 0\n",
      "42 : 0\n",
      "45 : 0\n",
      "50 : 0\n",
      "500 : 0\n",
      "60 : 0\n",
      "600 : 0\n",
      "65 : 0\n",
      "70 : 0\n",
      "75 : 0\n",
      "80 : 0\n",
      "800 : 0\n",
      "90 : 0\n",
      "900 : 0\n",
      "91 : 0\n",
      "92 : 0\n",
      "93 : 0\n",
      "95 : 0\n",
      "_the : 0\n",
      "ability : 0\n",
      "able : 1\n",
      "abortion : 0\n",
      "about : 1\n",
      "above : 0\n",
      "absolute : 0\n",
      "absolutely : 0\n",
      "ac : 0\n",
      "accept : 0\n",
      "acceptable : 0\n",
      "accepted : 0\n",
      "access : 0\n",
      "according : 0\n",
      "account : 0\n",
      "accurate : 0\n",
      "across : 0\n",
      "act : 0\n",
      "action : 0\n",
      "actions : 0\n",
      "active : 0\n",
      "activities : 0\n",
      "activity : 0\n",
      "acts : 0\n",
      "actual : 0\n",
      "actually : 0\n",
      "ad : 0\n",
      "add : 0\n",
      "added : 0\n",
      "addition : 0\n",
      "additional : 0\n",
      "address : 0\n"
     ]
    }
   ],
   "source": [
    "# 단어 빈도 출력\n",
    "for word, count in zip(\n",
    "    cv.get_feature_names_out()[:100], X_train_cv[0].toarray()[0, :100]\n",
    "):\n",
    "    print(word, \":\", count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나이브 베이즈 분류기를 이용한 문서 분류\n",
    "예를 들어 5개의 추출된 기사는 다음과 같다고 하자\n",
    "분류에 경제, 경제, 과학, 과학, 경제\n",
    "분류에 대한 특성은 각각 (인플레이션, 인플레이션, 브릭스, 4차산업혁명), (물가지수, 물가지수, GDP, 인플레이션, 브릭스), (4차산업혁명, 인공지능, 인공지능, 머신러닝), (인공지능, 우주산업, 우주산업, 딥러닝, 딥러닝), (4차산업혁명, 4차산업혁명, 우주산업, GDP)\n",
    "여기서 경제 기사는 5개 중 3개 이므로 3/5, 과학기사는 2/5\n",
    "학습데이터셋을 통해 어떤 단어가 경제 기사에 많이 나오는지 알 수 있다.\n",
    "즉 새로운 기사에 나온 단어들이 각각 어떤 확률로 나오는지를 계산하고, 이를 잘 결합할 수 있다면 기사가 어떤 분류에 속하는지를 알 수 있다.\n",
    "MultinomialNB\n",
    "이 클래스는 이산적인 특성 값들을 이용해 분류를 하고자 할 때 사용한다.\n",
    "이산적이란 연속적인 값이 아닌 값이라는 뜻으로 카운트 벡터가 여기에 해당한다.\n",
    "하지만 TF-IDF와 같이 연속적인 값에도 잘 작동한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 0.824\n",
      "테스트 세트 정확도: 0.732\n"
     ]
    }
   ],
   "source": [
    "# 나이브 베이지안을 통한 분류\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# 분류기 선언\n",
    "NB_clf = MultinomialNB()\n",
    "\n",
    "# train_set를 이용한 분류기 학습\n",
    "NB_clf.fit(X_train_cv, y_train)\n",
    "\n",
    "# 훈련 세트 정확도\n",
    "print(\"훈련 세트 정확도: {:.3}\".format(NB_clf.score(X_train_cv, y_train)))\n",
    "\n",
    "# 테스트 세트 정확도\n",
    "print(\"테스트 세트 정확도: {:.3}\".format(NB_clf.score(X_test_cv, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫번째 테스트 데이터와 라벨:  TRry the SKywatch project in  Arizona. 2\n",
      "두번째 테스트 데이터와 라벨:  The Vatican library recently made a tour of the US.\n",
      " Can anyone help me in finding a FTP site where this collection is \n",
      " available. 1\n",
      "예측 라벨: [2 1]\n",
      "예측 라벨 문자열: sci.space comp.graphics\n"
     ]
    }
   ],
   "source": [
    "# 주어진 텍스트에 대한 예측: predict() 메서드 사용\n",
    "# 예측에는 라벨은 필요 없으므로 특성 값만 인수로 전달\n",
    "# 결과는 라벨과 동일하게 숫자로 반환. 이를 실제 카테고리 명으로 알려면 newsgroups_train.target_names를 이용\n",
    "\n",
    "# topics:  ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
    "\n",
    "print(\"첫번째 테스트 데이터와 라벨: \", X_test[0], y_test[0])\n",
    "print(\"두번째 테스트 데이터와 라벨: \", X_test[1], y_test[1])\n",
    "\n",
    "pred = NB_clf.predict(X_test_cv[:2])\n",
    "print(\"예측 라벨:\", pred)\n",
    "print(\n",
    "    \"예측 라벨 문자열:\",\n",
    "    newsgroups_train.target_names[pred[0]],\n",
    "    newsgroups_train.target_names[pred[1]],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나이브 베이지안 모델 개선\n",
    "# max_features, mindf, maxdf 와 같은 매개변수 조정\n",
    "# 복잡도 조절하는 알파 값을 늘리면 통계 데이터가 완만해지고 복잡도가 낮아진다.\n",
    "# Tfidvectorizer 사용\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.5)\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 0.862\n",
      "테스트 세트 정확도: 0.741\n"
     ]
    }
   ],
   "source": [
    "# tfidf 훈련 세트를 이용한 분류기 재학습\n",
    "NB_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 훈련 세트 정확도\n",
    "print(\"훈련 세트 정확도: {:.3}\".format(NB_clf.score(X_train_tfidf, y_train)))\n",
    "\n",
    "# 테스트 세트 정확도\n",
    "print(\"테스트 세트 정확도: {:.3}\".format(NB_clf.score(X_test_tfidf, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 회귀분석을 이용한 문서의 분류\n",
    "회귀분석은 예측하고자 하는 값이 연속적일때 사용하는 반면\n",
    "로지스틱 회귀분석은 라벨이 연속적인 값이 아니고, 분류에 해당할 때 사용한다.\n",
    "분류가 이진일때, 즉 두 개일 때와 셋 이상의 다중 클래스일 때는 서로 다른 알고리즘을 사용한다.\n",
    "여기서는 학습할 분류가 4개의 토픽이므로 다중 클래스에 해당된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 0.929\n",
      "테스트 세트 정확도: 0.735\n"
     ]
    }
   ],
   "source": [
    "# 로지스틱 회귀분석\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 로지스틱 회귀 클래서 객체 선언 \n",
    "LR_clf = LogisticRegression()\n",
    "\n",
    "#tfidf로 벡터화한 데이터 학습 \n",
    "LR_clf.fit(X_train_tfidf , y_train)\n",
    "\n",
    "# 훈련 세트 정확도\n",
    "print(\"훈련 세트 정확도: {:.3}\".format(LR_clf.score(X_train_tfidf, y_train)))\n",
    "\n",
    "# 테스트 세트 정확도\n",
    "print(\"테스트 세트 정확도: {:.3}\".format(LR_clf.score(X_test_tfidf, y_test)))\n",
    "\n",
    "\n",
    "# 위 결과를 보면 나이브 베이즈보다 테스트 성능이 다소 떨어진다.\n",
    "# 그 이유로 첫째는 나이브 베이즈의 가정이 텍스트 분류의 환경과 잘 맞아서 나이브 베이즈가\n",
    "# 일반적으로 좋은 성능을 보여주기 때문이다.\n",
    "# 둘째로 하나의 가능성인데, 학습 데이터와 평가 데이터에 대한 성능의 차이가 로지스틱 회귀분석에서 더 큰 것을 볼 수 있다.\n",
    "# 이것은 로지스틱 회귀에서 과적합이 일어났을 가능성이 있음을 의미한다.\n",
    "# 이러한 과적합을 방지하는 방법으로 척째 특성의 수를 줄이는 것이고,\n",
    "# 둘째 정규화를 이용해 각 특성에 대한 계수가 지나치게 커지는 것을 방지하는 방법이 있다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 릿지 회귀를 이용한 과적합 방지\n",
    "\n",
    "\n",
    "- 릿지회귀는 회귀 분석에 정규화를 사용하는 알고리즘이다.\n",
    "- 최적화를 위한 목적함수에 정규화 항목을 넣어서 특성에 대한 계수가 지나치게 커지는 것을 억제한다\n",
    "- 참조: machine_learning > supervised learning > multiple regression 참조\n",
    "- 선 그래프에서 실제 데이터의 이동을 모두 연결하면 과적합된다.\n",
    "- 따라서 선이 급격하게 변경된다는 것은 기울기가 급격하게 변동하는 것이다.\n",
    "- 이를 방지하려면 계수가 지나치게 커지지 않게 계수에 제약을 가한다.\n",
    "- 이 방식 중 대표적인 알고리즘이 릿지 회귀다.\n",
    "- 릿지에서는 계수의 제곱을 규제하는데 이를 L2 규제라 한다.\n",
    "\n",
    "\n",
    "## RidgeClassifier의 alpha\n",
    "\n",
    "\n",
    "- 사이킷런에서 제공하는 RidgeClassifier 클래스의 매개변수로 alpha가 있다.\n",
    "- 이는 정규화 정도를 조절한다.\n",
    "- 값이 커질수록 정규화이 비중이 커져서 계수의 변화를 더 많이 억제한다.\n",
    "- 하지만 너무 커지면 학숩 자체가 잘 안되므로 적절할 값을 찾아야 한다.\n",
    "- 이러한 매개변수를 하이퍼파라이터라 한다.\n",
    "- 하이퍼 파라미터는 사용자가 적절히 정해야 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 0.96\n",
      "테스트 세트 정확도: 0.735\n"
     ]
    }
   ],
   "source": [
    "# 기본 값으로 릿지 회귀 구현\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "ridge_clf = RidgeClassifier()\n",
    "ridge_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 훈련 세트 정확도\n",
    "print('훈련 세트 정확도: {:.3}'.format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "\n",
    "# 테스트 세트 정확도\n",
    "print('테스트 세트 정확도: {:.3}'.format(ridge_clf.score(X_test_tfidf, y_test)))\n",
    "\n",
    "# 위 결과를 보면 크게 개선되지 않았다.\n",
    "# 따라서 alpha를 조정하여 개선을 시도한다.\n",
    "# 적절한 alpha 값을 찾기 위해서는 학습 데이터를 다시 분리해서 검증 데이터셋을 만든다.\n",
    "# 검증 데이터셋에 대한 성능이 최고가 되는 alpha 값을 선택해야 한다.\n",
    "# 평가 데이터셋을 사용하지 않는 이유는 평가 데이터셋은 오직 최종 평가에만 사용되어야 하기 때문이다.\n",
    "\n",
    "\n",
    "# 여기서는 원초적으로 그리드 서치를 구현한다.\n",
    "# 그리드 서치는 다양한 하이퍼 파라미터 값에 대한 검증 데이터셋에 대한 성능을 계산해\n",
    "# 비교함으로쏘ㅓ 적절한 하이퍼 파라미터 값을 얻기 위한 방법이다.\n",
    "# train_test_split을 이용해 학습데이터를 다시 분리한다.\n",
    "# alpha값을 0.1부터 10까지 0.1씩 증가시키면서 릿지 회귀분석을 실시하고 가장 큰 모형의 alpha를 찾는다\n",
    "# machine_learning > supervised learning > logistic regression 참조\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라쏘 회귀를 이용한 특성 선택\n",
    "라쏘 회귀는 특성의 계수에 대해 정규화 한다는 점에서는 유사하지만 정규화 항에 차이가 있다.\n",
    "릿지는 L2 정규화를 쓰는 반면, 라쏘는 L1 정규화를 사용한다.\n",
    "라쏘는 정규화를 할 때 특성의 계수가 0에 가까워지면 이를 완전히 0으로 바꾼다는 점에서 차이가 있다.\n",
    "어떤 특성의 계수가 0이라는 것은 그 특성은 실제로 분류에 전혀 영향을 미치지 않으며, 사실상 그 특성 값이 사용되지 않는다는 것을 의미한다.\n",
    "이는 특성의 수를 줄이는 결과를 가져온다.\n",
    "하지만 특성의 수가 줄어들면 정확도도 함께 줄어든다\n",
    "라쏘는 정규화를 통해 과적합을 줄이지만 동시에 특성의 수도 줄이므로 성능, 즉 정확도 향상은 된다고 보기 어렵다.\n",
    "사이킷런은 라쏘 회귀분석을 사용하는 별도의 분류기를 제공하지 않고 기존의 로지스틱 회귀분석에서 정규화 방식을 L1으로 선택하는 방식으로 사용된다.\n",
    "동시에 알고리즘도 'liblinear'를 선택해야 한다.\n",
    "릿지 회귀에서는 alpha를 올리면 정규화가 강재지지만 라쏘에서는 C는 값이 커지면 정규화가 작아진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 0.819\n",
      "테스트 세트 정확도: 0.724\n",
      "used feature count: 437\n"
     ]
    }
   ],
   "source": [
    "# C를 1로 지정하고 라쏘 모델 구현\n",
    "import numpy as np\n",
    "lasso_clf = LogisticRegression(solver=\"liblinear\", penalty=\"l1\", C=1)\n",
    "lasso_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 훈련 세트 정확도\n",
    "print(\"훈련 세트 정확도: {:.3}\".format(lasso_clf.score(X_train_tfidf, y_train)))\n",
    "\n",
    "# 테스트 세트 정확도\n",
    "print(\"테스트 세트 정확도: {:.3}\".format(lasso_clf.score(X_test_tfidf, y_test)))\n",
    "\n",
    "# 계ㅅ구 중에서 0이 아닌 단어의 개수 출력\n",
    "print(\n",
    "    \"used feature count: {}\".format(np.sum(lasso_clf.coef_ != 0)),\n",
    ")\n",
    "# 결과 과적합은 줄음 정확도 좀 낮아짐 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결정트리를 이용한 문서분류 방법\n",
    "\n",
    "\n",
    "- 결정트리 알고리즘도 문서 분류에 사용할 수 있다.\n",
    "- 결정트리의 장점 중 하나는 왜 그와 같이 예측했는지에 대해 체계적인 설명이 가능하다는 것이다.\n",
    "- 모형이 학습되면 결정트리를 그려서 분류가 되는 과정을 볼 수 있다.\n",
    "- 문제는 특성이 너무 많은 경우 모두 그려 보기 어렵다는 것이다.\n",
    "- 따라서 사용하려면 특성의 수를 비롯한 여러가지를 고려해야 한다.\n",
    "-----\n",
    "- 사이킷런은 결정트리를 위한 DecisionTreeClassifier클래스,\n",
    "- 결정트리 기반의 앙상블 모형 중 하나인 랜덤포레스트를 위한 RandomForestClassifier 클래스,\n",
    "- 결정트리 기반의 모형 중에서 높은 성능으로 알려진 글라디언트 부스팅을 지원하는 GradientBoostingClassifier 클래스를 제공한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결정트리 훈련 세트 정확도: 0.977\n",
      "결정트리 테스트 세트 정확도: 0.536\n",
      "--------------------------------------------------\n",
      "랜덤 포레스트 훈련 세트 정확도: 0.977\n",
      "랜덤 포레스트 테스트 세트 정확도: 0.685\n",
      "--------------------------------------------------\n",
      "글라디언트 부스팅 훈련 세트 정확도: 0.933\n",
      "글라디언트 부스팅 테스트 세트 정확도: 0.695\n"
     ]
    }
   ],
   "source": [
    "# 3가지 결정트리 모형 테스트\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=7)\n",
    "tree.fit(X_train_tfidf, y_train)\n",
    "print(\"결정트리 훈련 세트 정확도: {:.3}\".format(tree.score(X_train_tfidf, y_train)))\n",
    "print(\"결정트리 테스트 세트 정확도: {:.3}\".format(tree.score(X_test_tfidf, y_test)))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "forest = RandomForestClassifier(random_state=7)\n",
    "forest.fit(X_train_tfidf, y_train)\n",
    "print(\n",
    "    \"랜덤 포레스트 훈련 세트 정확도: {:.3}\".format(forest.score(X_train_tfidf, y_train))\n",
    ")\n",
    "print(\n",
    "    \"랜덤 포레스트 테스트 세트 정확도: {:.3}\".format(forest.score(X_test_tfidf, y_test))\n",
    ")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=7)\n",
    "gb.fit(X_train_tfidf, y_train)\n",
    "print(\n",
    "    \"글라디언트 부스팅 훈련 세트 정확도: {:.3}\".format(gb.score(X_train_tfidf, y_train))\n",
    ")\n",
    "print(\n",
    "    \"글라디언트 부스팅 테스트 세트 정확도: {:.3}\".format(gb.score(X_test_tfidf, y_test))\n",
    ")\n",
    "\n",
    "# 결과를 보면 결정트리와 랜덤 포레스트는 학습 데이터 결과가 가장 좋다.\n",
    "# 이는 학습 데이터에 과적합 되는 성향이 매우 강하다는 것 나타낸다\n",
    "# 글라디언트 부스팅으로 갈수록 성능이 높아진다.\n",
    "# 하지만 전체적인 성능은 좋지 않다.\n",
    "# 원인은 최적화를 위한 하이퍼 파라미터 탐색이 이뤄지지 않은 원인도 있지만,\n",
    "# 문서 분류의 경우에는 전반적으로 결정트리 기반의 알고리즘이 좋은 성능을 보여주지 못한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류 모델 성능을 높이는 방법\n",
    "\n",
    "\n",
    "- 정규화 등을 좀 더 세심하게 해볼 수 있다.\n",
    "- CountVectorizer와 TfidfVectorizer는 자체 토크라이저를 쓸 수도 있지만 외부에서 정의한 함수를 사용할 수 있으므로 앞의 내용을 바탕으로 토크나이저를 향상시킨다.\n",
    "- NLTK의 불용어 사전을 이용해 불용어를 제거하고 포터 스테머를 사용해 스태밍을 수행한다.\n",
    "- TfidfVectorizer에 이 토큰화 결과를 입력으로 사용해 나온 특성 벡터로 로지스틱 회귀분석을 실시하여 결과를 분석한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\3호실-09\\AppData\\Local\\Temp\\ipykernel_848\\3514025121.py:15: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  RegTok = RegexpTokenizer(\"[\\w']{3,}\")  # 특수 문자 제거하기 위한 토크라이징 정의\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\3호실-09\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\Anaconda\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 0.931\n",
      "테스트 세트 정확도: 0.75\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# 포터 스태머 참조 : 참조: https://skyjwoo.tistory.com/entry/Porter-Stemmer%ED%8F%AC%ED%84%B0-%EC%8A%A4%ED%85%8C%EB%A8%B8%EB%A5%BC-%EB%A7%8C%EB%93%A4%EC%96%B4%EB%B3%B4%EC%9E%90\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "RegTok = RegexpTokenizer(\"[\\w']{3,}\")  # 특수 문자 제거하기 위한 토크라이징 정의\n",
    "english_stops = set(stopwords.words(\"english\"))  # 영어 불용어 제거를 위한 정의\n",
    "\n",
    "\n",
    "# 토크나이징 커스텀 함수 정의\n",
    "def tokenizer(text):\n",
    "    tokens = RegTok.tokenize(text.lower())\n",
    "    # 불용어 제외\n",
    "    words = [word for word in tokens if (word not in english_stops) and len(word) > 2]\n",
    "    # 포터 스테머 적용\n",
    "    features = list(map(lambda token: PorterStemmer().stem(token), words))\n",
    "    return features\n",
    "\n",
    "\n",
    "# 새로 정의한 커스텀 토크나이징 함수를 tfidf 벡터화에 사용\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenizer, max_features=2000, min_df=5, max_df=0.5)\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# 로지스틱 회귀 분석\n",
    "LR_clf = LogisticRegression()\n",
    "LR_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 훈련 세트 정확도\n",
    "print(\"훈련 세트 정확도: {:.3}\".format(LR_clf.score(X_train_tfidf, y_train)))\n",
    "\n",
    "# 테스트 세트 정확도\n",
    "print(\"테스트 세트 정확도: {:.3}\".format(LR_clf.score(X_test_tfidf, y_test)))\n",
    "\n",
    "# 앞서 릿지 회귀의 정확도 0.74에 비해 향상됐다.\n",
    "# 여기서는 특성 수를 2000개로 제한했다.\n",
    "# 그 이유는 샘플 수가 2034개밖에 되지 않기 때문이다.\n",
    "# 카테고리가 4개인 로지스틱 회귀분석이라면 8000개의 계수를 2043개의 샘플로 추정해야 하는 상황이다.\n",
    "# 따라서 일반적으로는 어마어마한 과적합이 일어나서 일반화가 거의 안 돼야 하는데, 문서 분류에서는 별 문제 없이 가능하다\n",
    "# 그렇다면 특성의 수를 더 늘려보면 어떨까?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
