{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 토크나이징 ##\n",
    "\n",
    "\n",
    "- 컴퓨터 분야에서 자연어의 의미를 분석해 컴포터가 처리할 수 있도록 하는 일을 자연어처리 NLP(Natural Lanugage Precessing) 이라 한다.\n",
    "- 이를 위해 우선 문장을 일정한 의미가 있는 가장 작은 단어들로 나눈다.\n",
    "- 그 다음 나눠진 단어들을 이용해 의미를 분석한다.\n",
    "- 여기서 가장 기본이 되는 단어들을 토큰이라 한다.\n",
    "- 토큰의 단위는 토크나이징 방법에 따라 달라질 수 있지만 인ㄹ반적으로 일정한 의미가 있는 가장 작은 정보 단위로 결정된다.\n",
    "-----\n",
    "\n",
    "\n",
    "- 이와 같이 주어진 문장에서 토큰 단위로 정보를 나누는 작업을 토크나이징이라 한다.\n",
    "- 이는 문장 형태의 데이터를 처리하기 위해 제일 처음 수행해야 하는 기본적인 작업니다.\n",
    "- 주로 텍스트 전처리 과정에서 사용한다.\n",
    "- 한국어 토크나이징을 지원하는 라이브러리로 KoNLPy(코엔엘파이)가 유명하다.\n",
    "-----\n",
    "\n",
    "\n",
    "- 토크나이징 시 토큰 단위를 어떻게 정의하느냐에 따라 자연어 처리 성능에 영향을 미친다.\n",
    "- 여기서는 형태소를 토큰 단위로 사용한다. 형태소는 일정한 의미가 있는 가장 작은 말의 단위를 사용한다.\n",
    "- 한국어는 명사와 조사를 띄어쓰지 않고, 용언에 따라 여러가지 어미가 붙기 때문에 띄어쓰기만으로 토크나이징을 할 수 있다.\n",
    "- 따라서 형태소뿐만 아니라 어근, 접두사/접미사, 품사 등 다양한 언어적 속성의 구주를 파악해야 한다.\n",
    "- 형태소 분석기는 형태소 의미를 고려해 품사를 태깅한다.\n",
    "- 9품사 참조: https://blog.naver.com/hunminsam/222335335162\n",
    "-----\n",
    "\n",
    "\n",
    "- KoNLPy에서는 세 가지 종류의 형태소 분석기를 제공한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 형태소 분석기  Kkma(꼬꼬마 )\n",
    "# 꼬꼬마 패키지 설치 \n",
    "from konlpy.tag import Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 꼬꼬마 함수\n",
    "# morphs(phrase): 인자로 입력한 문장을 형태소 단위로 토크나이징한다. 결과는 리스트 형태로 반환한다.\n",
    "# nouns(pharas): 인자로 입력한 문장에서 품사가 명사인 토큰만 추출한다. 리스트 반환(요소가 튜플)\n",
    "# pos(phrase, flatten=True): POS tagger라 부르며, 인자로 입력한 문장에서 형태소를 추출한 뒤 품사를 태깅한다.\n",
    "# 결과로 품사가 튜플 형태로 묶여서 리스트로 반환된다.\n",
    "# sentences(phrase): 인자로 입력한 여러 문장을 분리해주는 역할을 한다. 결과는 리스트 형태로 반환된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['꽁꽁', '얼', 'ㄴ', '한강', '위', '에', '고양이', '가', '걸어다니', 'ㅂ니다', '.']\n"
     ]
    }
   ],
   "source": [
    "kkma = Kkma()\n",
    "\n",
    "text =\" 꽁꽁 언 한강 위에 고양이가 걸어다닙니다.\"\n",
    "\n",
    "# 형태소 추출 \n",
    "morphs = kkma.morphs(text)\n",
    "print(morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소와 품사 태그 추출\n",
    "# NNG: 일반 명사, JKS: 주격 조사, JKM: 부사격 조사, VV: 동사, EFN: 평서형 종결어미, SF: 마침표, 물음표, 느낌표\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('꽁꽁', 'MAG'), ('얼', 'VV'), ('ㄴ', 'ETD'), ('한강', 'NNP'), ('위', 'NNG'), ('에', 'JKM'), ('고양이', 'NNG'), ('가', 'JKS'), ('걸어다니', 'VV'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "pos = kkma.pos(text)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한강', '위', '고양이']\n"
     ]
    }
   ],
   "source": [
    "# 명사 추출 \n",
    "\n",
    "nouns = kkma.nouns(text)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘의 날씨 입니다.', '허벌나게 춥습니다.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 분리 \n",
    "sentences = '오늘의 날씨 입니다. 허벌나게 춥습니다.'\n",
    "s = kkma.sentences(sentences)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코모란(komoran)\n",
    "# morphs(phrase): 인자로 입력한 문장을 형태소 단위로 토크나이징. 결과를 리스트 형태로 반환\n",
    "# nouns(phrase): 인자로 입력한 문장에서 명사 토큰을 추출\n",
    "# pos(phrase, flatten=True): 인자로 입력한 문장에서 형태소를 추출한 뒤 품사를 태깅한다.\n",
    "\n",
    "from konlpy.tag import Komoran, Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객체 생성 \n",
    "\n",
    "# 객체 (STABLE 모델 사용)\n",
    "komoran = Komoran('STABLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['꽁꽁', '얼', 'ㄴ', '한강', '위', '에', '고양이', '가', '걷', '어', '다니', 'ㅂ니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석 \n",
    "\n",
    "morphs = komoran.morphs(text)\n",
    "print(morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('꽁꽁', 'MAG'), ('얼', 'VV'), ('ㄴ', 'ETM'), ('한강', 'NNP'), ('위', 'NNG'), ('에', 'JKB'), ('고양이', 'NNP'), ('가', 'JKS'), ('걷', 'VV'), ('어', 'EC'), ('다니', 'VV'), ('ㅂ니다', 'EF'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "post = komoran.pos(text)\n",
    "print(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한강', '위', '고양이']\n"
     ]
    }
   ],
   "source": [
    "# 명사만 추출 \n",
    "nouns = komoran.nouns(text)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소: ['꽁꽁', '언', '한강', '위', '에', '고양이', '가', '걸어', '다닙니다', '.']\n",
      "품사 태깅: [('꽁꽁', 'Noun'), ('언', 'Modifier'), ('한강', 'Noun'), ('위', 'Noun'), ('에', 'Josa'), ('고양이', 'Noun'), ('가', 'Josa'), ('걸어', 'Verb'), ('다닙니다', 'Verb'), ('.', 'Punctuation')]\n",
      "명사: ['꽁꽁', '한강', '위', '고양이']\n",
      "오늘 날씨가 좋아요ㅋㅋ\n",
      "['오늘', '오늘 날씨', '좋아욬', '날씨']\n"
     ]
    }
   ],
   "source": [
    "# Okt(Open source korean text processor)는 트위터에서 개발한 한국어 처리기다\n",
    "# OKT는 띄어 쓰기가 어느 정도 되어 있는 문장을 빠르게 분석할 때 많이 사용한다.\n",
    "# pos(phrase, stem=False, join=False): 인자로 입력한 문장에서 형태소를 추출한 뒤 품사를 태깅한다. 결과는 튜플 형태로 묶여 리스트로 반환된다.\n",
    "# normalize(phrase): 문장을 정규화 한다.\n",
    "\n",
    "\n",
    "# Okt 형태소 분석기 생성\n",
    "okt = Okt()\n",
    "\n",
    "\n",
    "# 분석할 텍스트\n",
    "test = \" 꽁꽁 언 한강 위에 고양이가 걸어다닙니다.\"\n",
    "\n",
    "\n",
    "# 형태소 분석\n",
    "morphs = okt.morphs(test)\n",
    "print(\"형태소:\", morphs)\n",
    "\n",
    "\n",
    "# 형태소와 품사 태그 추출\n",
    "pos = okt.pos(test)\n",
    "print(\"품사 태깅:\", pos)\n",
    "\n",
    "\n",
    "# 명사만 추출\n",
    "nouns = okt.nouns(test)\n",
    "print(\"명사:\", nouns)\n",
    "\n",
    "\n",
    "# 정규화 추출\n",
    "test = \"오늘 날씨가 좋아욬ㅋㅋ\"\n",
    "print(okt.normalize(test))\n",
    "print(okt.phrases(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('우리', 'NP'), ('챗봇은', 'NA'), ('엔', 'NNB'), ('엘', 'NNP'), ('피', 'NNG'), ('를', 'JKO'), ('좋아하', 'VV'), ('아', 'EC')]\n"
     ]
    }
   ],
   "source": [
    "# 미등록 형태소 \n",
    "komoran = Komoran()\n",
    "text = \"우리 챗봇은 엔엘피를 좋아해  \"\n",
    "\n",
    "pos = komoran.pos(text)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_dict.tsv 파일 내용:\n",
      "엔엘피\tNNG\n"
     ]
    }
   ],
   "source": [
    "# 코모란은 문장 내에 사전에 포함된 단어가 나오면 우선적으로 정의된 품사 태그를 붙인다.\n",
    "# 주로 사람이름, 지명, 인터넷 용어, 특수 용어 등이 고유명사를 인식하는데 활용된다.\n",
    "# 단어와 품사를 탭으로 구분해 준다.\n",
    "# user_dict.tsv 텍스트 내용 확인\n",
    "import os\n",
    "\n",
    "# 파일 경로 설정\n",
    "os.file_path = \"user_dict.tsv\"\n",
    "\n",
    "\n",
    "# 파일 내용 읽기\n",
    "try:\n",
    "    with open(os.file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "        print(\"user_dict.tsv 파일 내용:\")\n",
    "        print(content)\n",
    "except FileNotFoundError:\n",
    "    print(f\"파일 '{os.file_path}'이(가) 존재하지 않습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('우리', 'NP'), ('챗봇은', 'NA'), ('엔엘피', 'NNG'), ('를', 'JKO'), ('좋아하', 'VV'), ('아', 'EC')]\n"
     ]
    }
   ],
   "source": [
    "komoran = Komoran(userdic='user_dict.tsv')\n",
    "text = \"우리 챗봇은 엔엘피를 좋아해  \"\n",
    "\n",
    "pos = komoran.pos(text)\n",
    "print(pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 ##\n",
    "\n",
    "\n",
    "- 컴퓨터는 자연어를 직접적으로 처리할 수 없다.\n",
    "- 연산만 가능하기 때문에 자연어를 숫자나 벡터 형태로 변환해야 하는데 이를 임베딩(embedding)이라 한다.\n",
    "- 임베딩은 단어나 문장을 수치화해 벡터 공간으로 표현하는 과정이다.\n",
    "- 임베딩은 말뭉치의 의미에 따라 벡터화하기 때문에 문법적인 정보가 포함되어 있다.\n",
    "-----\n",
    "\n",
    "\n",
    "- 임베딩 기법에는 문장 임베딩과 단어 임베딩이 있다.\n",
    "- 문장 임베딩은 문장 전체를 벡터로 표현하는 방법이며, 단어 임베딩은 개별 단어를 벡터로 표현하는 방법이다.\n",
    "- 문장 임제딩은 문장의 전체 흐름을 파악해 벡터로 변화하기 때문에 문맥적 의미를 지니는 장점이 있다.\n",
    "- 하지만 문장은 길기 때문에 컴퓨터 자원 소모가 많다.\n",
    "- 단어 임베딩은 동음이의어에 대한 구분을 하지 않는다는 단점이 있다.\n",
    "- 하지만 학습 방법이 간단해 많이 사용된다.\n",
    "\n",
    "\n",
    "## 원-핫 인코딩 ##\n",
    "\n",
    "\n",
    "- 원 핫 인코딩은 단어를 숫자 벡터로 변환하는 가장 기본적인 방법이다.\n",
    "- 명칭에서도 알 수 있듯이 요소들 중 단 하나의 값만 1이고 나머지는 0인 인코딩을 의미한다.\n",
    "- 전체 요소 중 단 하나의 값만 1이기 때문에 희소(sparse) 벡터라 한다.\n",
    "- 인코딩을 하기 위해 단어 집합이라 불리는 사전을 먼저 만들어야 한다.\n",
    "- 말뭉치에 존재하는 모든 단어의 수가 차원을 결정한다.\n",
    "- 예를 들어 100개의 단어가 존재한다면 벡터의 크기는 100차원이 된다.\n",
    "- 단어 사전 내의 단어들은 각각 고유한 벡터를 가진다.\n",
    "- 예를 들어 ['오늘', '날씨', '구름']이 있을 때\n",
    "- 오늘: [1, 0. 0], 날씨: [0, 1, 0], 구름: [0, 0, 1]로 구분하는 방식이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '날씨', '구름']\n",
      "{'오늘': 0, '날씨': 1, '구름': 2}\n",
      "3\n",
      "[0, 1, 2]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "komoran = Komoran(\"STABLE\")\n",
    "text = \"오늘 날씨는 구름이 많아요.\"\n",
    "\n",
    "\n",
    "# 명사만 추출\n",
    "nouns = komoran.nouns(text)\n",
    "print(nouns)\n",
    "\n",
    "\n",
    "# 단어 사전 구축 및 단어별 인덱스 부여\n",
    "dicts = {}\n",
    "for word in nouns:\n",
    "    if word not in dicts.keys():\n",
    "        dicts[word] = len(dicts)\n",
    "\n",
    "\n",
    "print(dicts)\n",
    "\n",
    "\n",
    "# 원 핫 인코딩\n",
    "nb_classes = len(dicts)\n",
    "targets = list(dicts.values())\n",
    "one_hot_targets = np.eye(nb_classes)[targets]\n",
    "print(nb_classes)\n",
    "print(targets)\n",
    "print(one_hot_targets)\n",
    "\n",
    "\n",
    "# 2 * 2 단위 행렬 생성\n",
    "print(np.eye(2))\n",
    "\n",
    "\n",
    "# 3 * 3 단위 행렬 생성\n",
    "print(np.eye(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분산 표현 ##\n",
    "\n",
    "\n",
    "- 원 핫 인코딩은 간단한 구현 방법에 비해 좋은 성능을 가진다.\n",
    "- 하지만 원 핫 벡터의 경우 단순히 단어의 순서에 의한 인덱스 값을 기반으로 인코딩 된 값이기 때문에\n",
    "- 단어의 의미나 유사한 단어와의 관계를 담고 있지 않다.\n",
    "- 또한 차원이 커지면 메모리 낭비와 계산의 복잡도도 증가한다.\n",
    "- 위에서 살펴본 내용은 0과 1로만 이뤄진 희소 표현 방식이다.\n",
    "-----\n",
    "\n",
    "\n",
    "- 이를 효율적으로 해결할 수 있는 방법으로 분산 표현이 있다.\n",
    "- 분산 표현은 한 단어의 정보가 특정 차원에서 표현되지 않고, 여러 차원에 분산되어 표현된다.\n",
    "- 즉, 하나의 차원에 다양한 정보를 가진다.\n",
    "- 예를 들어 색상을 표현하는 RGB 모델은 3차원의 형태를 벡터로 생각할 수 있으며, 분산 표현의 한 방식이다.\n",
    "- 색상을 희소 표현으로 작성하먼 차원의 크기가 엄청나게 커지게 될 것이다.\n",
    "- 신경망을 사용할 때 분산 표현을 합습하는 과정에서 임베딩 벡터의 모든 차원에 의미있는 데이터를 고르게 밀집 시킨다.\n",
    "-----\n",
    "\n",
    "\n",
    "- 분산 표현의 장점으로 첫번째는 임베딩 벡터의 차원을 데이터 손실을 최소화 하면서 압축할 수 있다.\n",
    "- 희소 표현이 몇 천, 몇 만 차원의 크기를 생성한다면 분산 표현에서는 100 ~ 200차원 정도로 표현 할 수 있다.\n",
    "- 두 번째는 임베딩 벡터에는 단어의 의미, 주변 단어의 관계 등 많은 정보가 내포되어 일반화 능력이 뛰어나다.\n",
    "- 희소 표현에서는 '남자'와 '남성'은 그저 단 하나의 요솟값에 불과하다.\n",
    "- 하지만 분산 표현에서는 벡터 공간 상에서 유사한 의미를 갖는 단어들이 비슷한 위치에 분포되어 있기 때문에 '남자'와 '남성'의 단어 위치가 가깝다.\n",
    "- 이런 단어들의 거리를 계산할 수 있으며, 두 단어 사이의 유사성을 추론할 수 있게 된다.\n",
    "-----\n",
    "\n",
    "\n",
    "## Word2Vec ##\n",
    "\n",
    "\n",
    "- 분산 표현 형태의 단어 임베딩 모델로 제공되는 것이 Word2Vec다.\n",
    "- Word2Vec 모델은 CBOW(continuous bag of words)와 skip-gram 두 가지 모델로 제안된다.\n",
    "- CBOW모델은 맥락이라 표현되는 주변 단어들을 이용해 타겟 단어를 예측하는 신경망 모델이다.\n",
    "- 신경망의 입력을 주변 단어들로 구성하고 출력을 타겟 단어로 설정해 학습된 가중치 데이터를 임베딩 벡터로 활용한다.\n",
    "- skip-gram 모델은 타겟 단어를 이용해 주변 단어들을 예측한다.\n",
    "- 입출력이 CBOW 모델과 반대로 되어 있다.\n",
    "- 단어 분산 표현력이 우수하다고 평가되는 모델은 CBOW다.\n",
    "----\n",
    "\n",
    "\n",
    "- CBOW: 오늘(주변단어) ___(타겟단어: 예측 단어)는 구름(주변단어)이 많아요.\n",
    "- skip-gram: ___(주변단어: 예측 단어) 날씨(타겟단어)는 ___(주변단어: 예측 단어)이 많아요\n",
    "\n",
    "\n",
    "## 방향성 ##\n",
    "\n",
    "\n",
    "- Word2Vec는 밀집 벡터로 표현하며 학습을 통해 의미상 비슷한 단어들을 비슷한 벡터 공간에 위치시킨다.\n",
    "- 벡터 특성상 의미에 따라 방향성을 가진다.\n",
    "- 임베딩된 벡터들 간 연산이 가능하기 때문에 단어간 관계를 계산할 수 있다.\n",
    "\n",
    "\n",
    "## 모델 사용을 위한 설정 ##\n",
    "- Word2Vec 모델은 텐서플로나 케라스 같은 신경망 라이브러리를 사용할 필요가 없다.\n",
    "- 여기서는 토픽 모델링과 자연어 처리를 위한 Gensim 패키지를 사용한다.\n",
    "- 한국어에 대한 분석을 위해 한국어 말뭉치를 수집해야 한다.\n",
    "- 말뭉치 데이터는 양이 많기 때문에 모델을 학습하는데 시간이 걸린다.\n",
    "- 이를 매번 할 수 없으므로 각 단어의 임베딩 벡터가 설정돼 있는 모델을 파일로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 패키지 \n",
    "from gensim.models import Word2Vec\n",
    "import time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 말뭉치 데이터 읽기 시작\n",
      "전체 리뷰 개수: 200000\n",
      "말뭉치 데이터 읽기 완료: 0.75760817527771\n",
      "2. 형태소에서 명사만 추출 시작\n",
      "2. 형태소에서 명사만 추출 완료: 108.1187674999237\n",
      "3. Word2Vec 모델 학습 시작\n",
      "3. Word2Vec 모델 학습 완료: 128.6343240737915\n",
      "4. 학습된 모델 저장 시작\n",
      "4. 학습된 모델 저장 완료: 129.3103232383728\n",
      "corpus_count: 184991\n",
      "corpus_total_words: 1076896\n"
     ]
    }
   ],
   "source": [
    "# 네이버 영화 리뷰 데이터 로드\n",
    "def read_review_data(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [line.split(\"\\t\") for line in f.read().splitlines()]\n",
    "        data = data[1:]  # 헤더 제거\n",
    "    return data\n",
    "\n",
    "\n",
    "# 학습 시간 측정\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# 리뷰 파일 읽기\n",
    "print(\"1. 말뭉치 데이터 읽기 시작\")\n",
    "review_data = read_review_data(\"ratings.txt\")\n",
    "\n",
    "\n",
    "print(f\"전체 리뷰 개수: {len(review_data)}\")\n",
    "print(f\"말뭉치 데이터 읽기 완료: {time.time() - start}\")\n",
    "\n",
    "\n",
    "# 문장 단위로 명사만 추출해 학습 입력 데이터로 작성\n",
    "print(\"2. 형태소에서 명사만 추출 시작\")\n",
    "komoran = Komoran(\"STABLE\")\n",
    "\n",
    "\n",
    "# 유효한 문장만 필터링\n",
    "docs = []\n",
    "for sentence in review_data:\n",
    "    if (\n",
    "        len(sentence) > 1 and sentence[1] and sentence[1].strip()\n",
    "    ):  # 문장이 비어 있지 않은지 확인\n",
    "        try:\n",
    "            nouns = komoran.nouns(sentence[1].strip())  # 명사 추출\n",
    "            if nouns:  # 명사가 있으면 추가\n",
    "                docs.append(nouns)\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {e}, 문장: {sentence[1]}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "print(f\"2. 형태소에서 명사만 추출 완료: {time.time() - start}\")\n",
    "\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "print(\"3. Word2Vec 모델 학습 시작\")\n",
    "model = Word2Vec(sentences=docs, vector_size=200, window=4, hs=1, min_count=2, sg=1)\n",
    "print(f\"3. Word2Vec 모델 학습 완료: {time.time() - start}\")\n",
    "\n",
    "\n",
    "# 모델 저장\n",
    "print(\"4. 학습된 모델 저장 시작\")\n",
    "model.save(\"nvmc.model\")\n",
    "print(f\"4. 학습된 모델 저장 완료: {time.time() - start}\")\n",
    "\n",
    "\n",
    "# 학습된 말뭉치 수, 코퍼스 내 전체 단어 수\n",
    "print(\"corpus_count:\", model.corpus_count)\n",
    "print(\"corpus_total_words:\", model.corpus_total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_total words:1076896\n",
      "사랑:[ 0.20528089 -0.24212056  0.01199546  0.0312775  -0.16841759 -0.46529567\n",
      "  0.05575024  0.1397094   0.07820108 -0.07852884 -0.28224036 -0.07223487\n",
      "  0.3217716   0.252906   -0.32570013  0.19494899 -0.23567207 -0.24604563\n",
      " -0.1606209  -0.3117657  -0.0563697   0.11310954 -0.17002252 -0.0619804\n",
      " -0.20848726 -0.2246089   0.11697464  0.01439617 -0.11968279 -0.00940413\n",
      "  0.0742961   0.25725642 -0.17599645 -0.21276434  0.09470383  0.13967702\n",
      " -0.04583268  0.09325586 -0.04738257  0.15360533 -0.2471918   0.02165564\n",
      "  0.0297426  -0.50118816  0.51823217  0.21158896 -0.19345015  0.08323443\n",
      "  0.3947066  -0.19431996  0.05266386 -0.13491166 -0.13825648 -0.17094095\n",
      "  0.14874913  0.26764667  0.12808758 -0.18462496  0.20844084 -0.17591049\n",
      " -0.04008162  0.03369572 -0.23016956  0.13915162  0.13524051 -0.15266696\n",
      " -0.24034084  0.09461023 -0.23596123  0.4688566  -0.07391874 -0.2217302\n",
      "  0.06613965  0.01056861 -0.025342   -0.09898705  0.27591708 -0.19546813\n",
      " -0.23997319 -0.04243538 -0.19276702  0.03811494  0.07172889  0.39125666\n",
      " -0.3792178  -0.06569596 -0.11526076 -0.00442789 -0.07950961 -0.16155095\n",
      " -0.02116915  0.22006714  0.0032393  -0.14132345  0.59094715 -0.03215029\n",
      "  0.12093106 -0.28051534 -0.15566656  0.01010717 -0.3273708   0.45514902\n",
      "  0.35952199 -0.00119996 -0.07207886 -0.27017683 -0.32968822  0.32886958\n",
      " -0.09351195 -0.47898895  0.04891268 -0.41687855 -0.08596448  0.04316792\n",
      " -0.14238925  0.12388634 -0.15729542 -0.16566575  0.07014193 -0.1717124\n",
      "  0.24216674  0.08610196 -0.15813997 -0.2458817   0.23606454  0.21420029\n",
      "  0.00300816  0.23866463 -0.09219322  0.09184405 -0.0615054   0.26413974\n",
      " -0.11381938  0.00291186 -0.06503361  0.1641543   0.35697827 -0.05865547\n",
      " -0.02729738 -0.07064871  0.09516157 -0.21912086 -0.12779877  0.01818234\n",
      "  0.25806242  0.00873029 -0.14691325 -0.22683175  0.06067007  0.05095887\n",
      " -0.13166878  0.00448652  0.06501824  0.12289523 -0.287086   -0.12895264\n",
      "  0.31843853 -0.02695577  0.21957828 -0.1759772  -0.17678916 -0.12195721\n",
      " -0.22405644 -0.20149477 -0.24695778  0.21804756 -0.17179228 -0.05895854\n",
      " -0.18142878 -0.01969016 -0.37540558 -0.08256741  0.33340093 -0.09259182\n",
      " -0.04616398  0.08975042  0.15970543 -0.14383173  0.26127008 -0.125588\n",
      "  0.0580471  -0.27108222  0.12146465 -0.19927715  0.00967563  0.10106426\n",
      " -0.08959411  0.00106023  0.23569077  0.22495025  0.26015788 -0.57458997\n",
      "  0.09641954 -0.3002868   0.33557215 -0.35765284 -0.20397902  0.21758465\n",
      "  0.0864076   0.07285374]\n"
     ]
    }
   ],
   "source": [
    "# 모델 로딩 \n",
    "model = Word2Vec.load('nvmc.model')\n",
    "print(f'corpus_total words:{model.corpus_total_words}' )\n",
    "\n",
    "# 사랑이라는 단어로 생성한 단어 임베딩 \n",
    "print(f'사랑:{model.wv['사랑']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일요일 : 월요일\t 0.6855121\n",
      "안성기 : 배우\t 0.6061248\n",
      "대기업 : 삼성\t 0.51622355\n",
      "일요일 : 삼성\t 0.23156446\n",
      "히어로 : 삼성\t 0.15564634\n"
     ]
    }
   ],
   "source": [
    "# 단어 유사도 계산\n",
    "print(\"일요일 : 월요일\\t\", model.wv.similarity(w1=\"일요일\", w2=\"월요일\"))\n",
    "print(\"안성기 : 배우\\t\", model.wv.similarity(w1=\"안성기\", w2=\"배우\"))\n",
    "print(\"대기업 : 삼성\\t\", model.wv.similarity(w1=\"대기업\", w2=\"삼성\"))\n",
    "print(\"일요일 : 삼성\\t\", model.wv.similarity(w1=\"일요일\", w2=\"삼성\"))\n",
    "print(\"히어로 : 삼성\\t\", model.wv.similarity(w1=\"히어로\", w2=\"삼성\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('연기자', 0.7240622639656067), ('다나와', 0.6859080195426941), ('김정학', 0.6801241040229797), ('김갑수', 0.6697471737861633), ('연기', 0.6607134342193604)]\n",
      "[('이승엽', 0.6763555407524109), ('기업', 0.6665073037147522), ('악덕', 0.6593935489654541), ('조계', 0.659226655960083), ('관광버스', 0.6584842205047607)]\n"
     ]
    }
   ],
   "source": [
    "# 가장 유사한 단어 추출\n",
    "print(model.wv.most_similar(\"배우\", topn=5))\n",
    "print(model.wv.most_similar(\"삼성\", topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 유사도 ##\n",
    "\n",
    "\n",
    "- 자연어 처리에서 문장 간의 의미가 얼마나 유사한지 계산해야 한다.\n",
    "- 임베딩 기법을 사용하면 컴퓨터는 두 문장 간의 유사도를 계산할 수 있다.\n",
    "- 앞선 내용과 다른 점은 단어 간 유사도가 아니라 문장간 유사도를 계산한다는 것이다.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "- 문장 간의 유사도를 계산하기 위해서는 문장 내에 존재하는 단어들을 수치화 해야 한다.\n",
    "- 여기서는 통계적인 방법을 이용해 유사도를 계산한다.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "## n-gram 유사도 ##\n",
    "\n",
    "\n",
    "- n-gram은 주어진 문장에서 n개의 연속적인 단어 시퀀스(단어 나열)를 의미한다.\n",
    "- 문장에서 n개의 단어를 토큰으로 사용한다.\n",
    "- 이웃한 단어의 출현 횟수를 통계적으로 표현해 텍스트 유사도를 계산하는 방법이다.\n",
    "- 단어의 출현 빈도에 기반한 유사도를 계산할 수 있으며, 이를 통해 논문 인용이나 정도를 조사할 수 있다.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "## 예시 ##\n",
    "- 1661년 6월 뉴턴은 선생님의 제안으로 트리니티에 입학하였다.\n",
    "- n=1: 1661년 / 6월 / 뉴턴 / 선생님 / 제안 / 트리니티 / 입학(unigram)\n",
    "- n=2: 1661년 6월 / 6월 뉴턴 / 뉴턴 선생님 / 선생님 제안 / 제안 트리니티 / 트리니티 입학(bigram)\n",
    "- n=3: 1661년 6월 뉴턴 / 6월 뉴턴 선생님 / 뉴턴 선생님 제안 / 선생님 제안 트리니티 / 제안 트리니티 입학(trigram)\n",
    "- n=4: 1661년 6월 뉴턴 선생님 / 6월 뉴턴 선생님 제안 / 뉴턴 선생님 제안 트리니티 / 선생님 제안 트리니티 입학(4-gram)\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "- 해당 문장을 토큰으로 분리한 후 단어 문서 행렬을 만든다.\n",
    "- 이후 두 문장을 서로 비교해 동일한 단어의 출현 빈도를 확률로 계산해 유사도를 구한다.\n",
    "- 수식 simil = tf(A, B) / tokens(A)\n",
    "- tf(A, B): 두 문장 A, B에서 동일한 토큰의 출현 빈도\n",
    "- tokens(A): A 문장에서 전체 토큰 수\n",
    "- 기준이 되는 문장 A에서 나온 전체 토큰 중에서 A와 B에 동일한 토큰이 얼마나 있는지 비율로 표시\n",
    "- 1에 가까울수록 B가 A에 유사하다고 평가\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "## 2-gram을 이용한 유사도 예시 #\n",
    "\n",
    "\n",
    "- A: 6월에 뉴턴은 선생님의 제안으로 트리니티에 입학했다.\n",
    "- B: 6월에 뉴턴은 선생님의 제안으로 대학교에 입학했다.\n",
    "- 문장 A는 6개의 토큰으로 구성된다.\n",
    "- 문장 B와 A가 유사한 토큰은 4개로 구성된다.\n",
    "\n",
    "\n",
    "- 6월, 뉴턴: 1\n",
    "- 뉴턴, 선생님: 1\n",
    "- 선생님, 제안: 1\n",
    "- 입학: 1\n",
    "- 제안, 트리니티: 0\n",
    "- 트리니티, 입학: 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어절 단위 n-gram: 추출된 토큰들은 튜플 형태로 반환\n",
    "def word_ngram(bow, num_gram):\n",
    "    text = tuple(bow)\n",
    "    ngrams = [text[x : x + num_gram] for x in range(0, len(text))]\n",
    "    return tuple(ngrams)\n",
    "\n",
    "\n",
    "# 유사도 계산\n",
    "# doc1의 토큰이 doc2의 토큰과 얼마나 동일한지 횟수를 카운트\n",
    "# 카운트 된 값을 doc1의 전체 토큰 수로 나누면 유사도가 계산된다.\n",
    "# 이 결과가 1에 가까울수록 doc1과 유사해진다.\n",
    "def similarity(doc1, doc2):\n",
    "    cnt = 0\n",
    "    for token in doc1:\n",
    "        if token in doc2:\n",
    "            cnt = cnt + 1\n",
    "    return cnt / len(doc1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 정의\n",
    "sentence1 = \"6월 뉴턴은 선생님의 제안으로 트리니티에 입학했다.\"\n",
    "sentence2 = \"6월 뉴턴은 선생님의 제안으로 대학교에 입학했다.\"\n",
    "sentence3 = \"나는 맛있는 밥을 뉴턴 선생님과 함께 먹었다.\"\n",
    "\n",
    "\n",
    "# 형태소 분석기에서 명사 추출\n",
    "komoran = Komoran(\"STABLE\")\n",
    "bow1 = komoran.nouns(sentence1)\n",
    "bow2 = komoran.nouns(sentence2)\n",
    "bow3 = komoran.nouns(sentence3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('6월', '뉴턴'), ('뉴턴', '선생님'), ('선생님', '제안'), ('제안', '트리니티'), ('트리니티', '입학'), ('입학',))\n",
      "(('6월', '뉴턴'), ('뉴턴', '선생님'), ('선생님', '제안'), ('제안', '대학교'), ('대학교', '입학'), ('입학',))\n",
      "(('밥', '뉴턴'), ('뉴턴', '선생'), ('선생', '님과 함께'), ('님과 함께',))\n"
     ]
    }
   ],
   "source": [
    "# 단어 n-gram 토큰 추출 - 2-gram 방식으로 추출\n",
    "doc1 = word_ngram(bow1, 2)\n",
    "doc2 = word_ngram(bow2, 2)\n",
    "doc3 = word_ngram(bow3, 2)\n",
    "\n",
    "\n",
    "# 추출된 토큰 출력\n",
    "print(doc1)\n",
    "print(doc2)\n",
    "print(doc3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# 유사도 계산\n",
    "r1 = similarity(doc1, doc2)\n",
    "r2 = similarity(doc3, doc1)\n",
    "\n",
    "\n",
    "# 유사도 출력\n",
    "print(r1)\n",
    "print(r2)\n",
    "\n",
    "\n",
    "# n을 크게 잡을수록 비교 문장의 토큰과 비교할 때 카운트를 놓칠 확률이 커진다.\n",
    "# n을 작게 잡을수록 카운트 확률은 높아지지만 문맥을 파악하는 정확도는 떨어질 수 밖에 없는 구조다.\n",
    "# 보통 1 ~ 5 사이의 값을 많이 사용한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코사인 유사도 ##\n",
    "\n",
    "\n",
    "- 단어나 문장을 벡터로 표할할 수 있다면 벡터 간 거리나 각도를 이용해 유사성을 파악할 수 있다.\n",
    "- 코사인 유사도는 두 벡터 간 코사인 각도를 이용해 유사도를 측정하는 방법이다.ㅁ\n",
    "- 이는 일반적으로 벡터의 크기가 중요하지 않을 때 그 그리를 측정하기 위해 사용한다.\n",
    "- 예를 들어 출현 빈도를 통해 유사도 계산을 한다면 동일한 단어가 많이 포함되어 있을수록 벡터의 크기가 커진다.\n",
    "- 이 때 코사인 유사도는 벡터의 크기와 상관없이 결과가 안정적이다.\n",
    "- 코사인은 -1 ~ 1 사이의 값을 가지며, 두 벡터의 방향이 완전히 동일한 경우에는 1, 반대 방향인 경우에는 -1에 가까워진다.\n",
    "\n",
    "\n",
    "## 예시 ##\n",
    "\n",
    "\n",
    "- A: 6월에 뉴턴은 선생님의 제안으로 트리니티에 입학했다.\n",
    "- B: 6월에 뉴턴은 선생님의 제안으로 대학교에 입학했다.\n",
    "- 6월, 뉴턴: 1\n",
    "- 뉴턴, 선생님: 1\n",
    "- 선생님, 제안: 1\n",
    "- 입학: 1\n",
    "- 제안, 트리니티: 0\n",
    "- 트리니티, 입학: 0\n",
    "- A = [1, 1, 1, 1, 1, 1, 0]\n",
    "- B = [1, 1, 1, 1, 0, 1, 1]\n",
    "-----\n",
    "- 분모\n",
    "- A|B = sigma(n, n=1)Ai * Bi\n",
    "- = (1*1) + (1*1) + (1*1) + (1*1) + (1*0) + (1*1) + (0*1)\n",
    "- = 1 + 1 + 1 + 1 + 0 + 1 + 0\n",
    "- = 5\n",
    "-----\n",
    "- 분자\n",
    "- ||A||||B|| = root(sigma(n, n=1)Ai^2) * root(sigma(n, n=1)Bi^2)\n",
    "- = root(1^2 + 1^2 + 1^2 + 1^2 + 1^2 + 1^2 + 0^2 +) * root(1^2 + 1^2 + 1^2 + 1^2 + 0^2 + 1^2 + 1^2 +)\n",
    "- = root(6) * root(6)\n",
    "- = root(36) = 6\n",
    "-----\n",
    "- 5 / 6 = 0.83333333\n",
    "- 문장 A와 B는 83%의 유사도를 가진다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot  # 인자로 들어온 2개의 넘파이 배열을 내적곱한다\n",
    "\n",
    "# 코사인 유사도 수식의 분모에 있는 벡터 크기를 계산하는 부분과 동일하기 때문에 벡터의 크기 계산에 노름을 사용한다.\n",
    "from numpy.linalg import (\n",
    "    norm,\n",
    ")  # 노름 함수를 사용한다. 여기서는 유클리드 노름을 사용한다.\n",
    "\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "def cos_sim(vec1, vec2):\n",
    "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "\n",
    "# TDM: 비교 문장에서 추출한 단어 사전을 기준으로 문장에 해당 단어들이 얼마나\n",
    "# 포함됐는지 나타내는 단어 문서 행렬을 만든다\n",
    "def make_tem_doc_mat(sentence_bow, word_dics):\n",
    "    freq_mat = {}\n",
    "\n",
    "    for word in word_dics:\n",
    "        freq_mat[word] = 0\n",
    "\n",
    "    for word in word_dics:\n",
    "        if word in sentence_bow:\n",
    "            freq_mat[word] += 1\n",
    "\n",
    "    return freq_mat\n",
    "\n",
    "\n",
    "# 단어 벡터 생성\n",
    "def make_vector(tdm):\n",
    "    vec = []\n",
    "    for key in tdm:\n",
    "        vec.append(tdm[key])\n",
    "\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 정의\n",
    "sentence1 = \"6월 뉴턴은 선생님의 제안으로 트리니티에 입학했다.\"\n",
    "sentence2 = \"6월 뉴턴은 선생님의 제안으로 대학교에 입학했다.\"\n",
    "sentence3 = \"나는 맛있는 밥을 뉴턴 선생님과 함께 먹었다.\"\n",
    "\n",
    "\n",
    "# 형태소 분석기에서 명사 추출\n",
    "komoran = Komoran(\"STABLE\")\n",
    "bow1 = komoran.nouns(sentence1)\n",
    "bow2 = komoran.nouns(sentence2)\n",
    "bow3 = komoran.nouns(sentence3)\n",
    "\n",
    "# 단어 묶음 리스트를 하나로 합치기 \n",
    "bow = bow1 + bow2 + bow3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 묶음에서 중복을 제거해 단어 사전 구축\n",
    "word_dics = []\n",
    "for token in bow:\n",
    "  if token not in word_dics:\n",
    "    word_dics.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'6월': 1, '뉴턴': 1, '선생님': 1, '제안': 1, '트리니티': 1, '입학': 1, '대학교': 0, '밥': 0, '선생': 0, '님과 함께': 0}\n",
      "{'6월': 1, '뉴턴': 1, '선생님': 1, '제안': 1, '트리니티': 0, '입학': 1, '대학교': 1, '밥': 0, '선생': 0, '님과 함께': 0}\n",
      "{'6월': 0, '뉴턴': 1, '선생님': 0, '제안': 0, '트리니티': 0, '입학': 0, '대학교': 0, '밥': 1, '선생': 1, '님과 함께': 1}\n"
     ]
    }
   ],
   "source": [
    "# 문장별 단어 문서 행렬 계산\n",
    "freq_list1 = make_tem_doc_mat(bow1, word_dics)\n",
    "freq_list2 = make_tem_doc_mat(bow2, word_dics)\n",
    "freq_list3 = make_tem_doc_mat(bow3, word_dics)\n",
    "print(freq_list1)\n",
    "print(freq_list2)\n",
    "print(freq_list3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333335\n",
      "0.20412414523193154\n"
     ]
    }
   ],
   "source": [
    "# 문장 벡터 생성\n",
    "doc1 = np.array(make_vector(freq_list1))\n",
    "doc2 = np.array(make_vector(freq_list2))\n",
    "doc3 = np.array(make_vector(freq_list3))\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "r1 = cos_sim(doc1, doc2)\n",
    "r2 = cos_sim(doc3, doc1)\n",
    "print(r1)\n",
    "print(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
